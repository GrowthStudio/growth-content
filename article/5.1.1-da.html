<h2 id="数据分析">数据分析</h2>
<p>数据分析是一个很有意思的过程，我们可以简单地将这个过程分成四个步骤：</p>
<ul>
<li>识别需求</li>
<li>收集数据</li>
<li>分析数据</li>
<li>展示数据</li>
</ul>
<p>值得注意的是：在分析数据的过程中，需要不同的人员来参与，需要跨域多个领域的知识点——分析、设计、开发、商业和研究等领域。因此，在这样的领域里，回归敏捷也是一种不错的选择（源于：《敏捷数据科学》）：</p>
<ul>
<li>通才高于专长</li>
<li>小团队高于大团队</li>
<li>使用高阶工具和平台：云计算、分布式系统、PaaS</li>
<li>持续、迭代地分享工作成果，即使这些工作未完成</li>
</ul>
<h3 id="识别需求">识别需求</h3>
<p>在我们开始分析数据之前，我们需要明确一下，我们的问题是什么？即，我们到底要干嘛，我们想要的内容是什么。</p>
<blockquote>
<p>识别信息需求是确保数据分析过程有效性的首要条件，可以为收集数据、分析数据提供清晰的目标。</p>
</blockquote>
<p>当我们想要提到我们的网站在不同的地区的速度时，我们就需要去探索我们的用户主要是在哪些地区。即，现在这是我们的需求。我们已经有了这样的一个明确的目标，下面要做起来就很轻松了。</p>
<h3 id="收集数据">收集数据</h3>
<p>那么现在新的问题来了，我们的数据要从哪里来？</p>
<p>对于大部分的网站来说，都会有访问日志。但是这些访问日志只能显示某个 IP 进入了某个页面，并不人详细地介绍这个用户在这个页面待了多久，做了什么事。这时候，这些数据就需要依赖于类似于 Google Analytics 这样的工具来统计网站的流量。还有类似于New Relic这样的工具来统计用户的一些行为。</p>
<p>在一些以科学研究为目的的数据收集中，我们可以从一些公开的数据中获取这些资料。</p>
<p>而在一些特殊的情况里，我们就需要通过爬虫来完成这样的工作。</p>
<h3 id="分析数据">分析数据</h3>
<p>现在，我们终于可以真正的去分析数据了——我的意思是，我们要开始写代码了。从海量的数据中过滤出我们想要的数据，并通过算法来对其进行分析。</p>
<p>一般来说，我们都利用现有的工具来完成大部分的工作。要使用哪一类工具，取决于我们如要分析的数据的数量级了。如果只是一般的数量级，我们可以考虑用 R 语言、Python、Octave 等单机工具来完成。如果是大量的数据，那么我们就需要考虑用 Hadoop、Spark 来完成这个级别的工作。</p>
<p>而一般来说，这个过程可能是要经过一系列的工具才能完成。如在之前我在分析我的博客的日志时(1G左右)，我用 Hadoop + Apache Pig + Jython 来将日志中的 IP 转换为 GEO 信息，再将 GEO 信息存储到 ElasticSearch 中。随后，我们就可以用 AMap、leaflet 这一类 GEO 库将这些点放置到地图上。</p>
<h3 id="展示数据">展示数据</h3>
<p>现在，终于来到我最喜欢的环节了，也是最有意思，但是却又最难的环节。</p>
<p>我们过滤后我们的数据，得到我们想要的内容后，我们就要去考虑如何可视化我们的数据。在我熟悉的 Web GIS领域里，我可以可视化出我过滤后的那些数据。但是对于我不熟悉的领域，要可视化这些数据不是一件容易的事。在少数情况下，我们才能使用现有的工具完成需求，多数情况下，我们也需要写相当的代码才能将数据最后可视化出来。</p>
<p>而在以什么形式来展示我们的数据时，又是一个问题。如一般的数据结果，我们到底是使用柱形图、条形图、折线图和面积图中的哪一种？这依赖于我们有一些 UX 方面的经验。</p>
<p>参考来源: <strong>精益数据分析</strong>。</p>
